{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection with Amazon SageMaker\n",
    "\n",
    "This notebook provides an overview at how Amazon SageMaker effortlessly simplifies the machine learning development cycle for even the most complex of models, such as the Single Shot Detector (SSD) for Face Detection.\n",
    "\n",
    "Amazon SageMaker is a fully-managed service that simplifies the process of **building**, **training**, and **deploying** machine learning and deep learning models at scale. It consists of three components:\n",
    "\n",
    "* **Build**: A fully managed Jupyter Notebook instance for loading data, prototyping models, and launching training jobs and deployment endpoints. This comes pre-configured with all of the standard deep learning libraries for both CPU and GPU computation. \n",
    "* **Train**: With one-click, SageMaker launches fully managed, scalable training jobs on AWS for both proprietary SageMaker ML algorithms and custom-designed deep learning architectures.\n",
    "* **Deploy**: With one-click SageMaker provides a fully managed service for deploying trained models to endpoint instances to host model inference as an API call.\n",
    "\n",
    "### Set-up\n",
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.mxnet import MXNet, MXNetModel\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import urllib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://sagemaker-obj-det-demo/mxnet_ssd_face_detection.py .\n",
    "!aws s3 cp s3://sagemaker-obj-det-demo/1_Handshaking_Handshaking_1_203.jpg .\n",
    "!aws s3 cp s3://sagemaker-obj-det-demo/19_Couple_Couple_19_187.jpg ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build\n",
    "\n",
    "The SageMaker Notebook instance is a convenient platform to explore data for a given task and preprocess if necessary.\n",
    "We can use this environment to easily launch training jobs using this data, and deploy trained models for inference.\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "Our model is tasked with Face Detection. Let's explore the data provided to us to understand how such a model might work.\n",
    "\n",
    "The data set the model will be trained on is a set of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_array = np.array(Image.open('1_Handshaking_Handshaking_1_203.jpg'))\n",
    "plt.imshow(img_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a corresponding set of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1--Handshaking/1_Handshaking_Handshaking_1_203.jpg\n",
    "# 4\n",
    "# 142.756032 258.058981 112.557641 216.879357\n",
    "# 241.587131 433.758713 90.595174 161.973190\n",
    "# 691.817694 387.088472 82.359249 178.445040\n",
    "# 782.412869 527.099196 85.104558 175.699732"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers here describe bounding boxes for the four faces in the image.\n",
    "\n",
    "Each row represents: <tt>left top width height</tt>. Let's put these together with the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = [\n",
    "         [int(np.round(142.756032)), int(np.round(258.058981)), int(np.round(112.557641)), int(np.round(216.879357))],\n",
    "         [int(np.round(241.587131)), int(np.round(433.758713)), int(np.round(90.595174)), int(np.round(161.973190))],\n",
    "         [int(np.round(691.817694)), int(np.round(387.088472)), int(np.round(82.359249)), int(np.round(178.445040))],\n",
    "         [int(np.round(782.412869)), int(np.round(527.099196)), int(np.round(85.104558)), int(np.round(175.699732))]\n",
    "        ]\n",
    "for box in boxes:\n",
    "    img_array = cv2.rectangle(img_array,(box[0],box[1]),(box[0]+box[2],box[1]+box[3]),(0,255,0),10)\n",
    "plt.imshow(img_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like image classification, face detections is a supervised training task. Here, the inputs are the images and the outputs are the bounding box coordinates.\n",
    "\n",
    "MXNet lets us convert images into a binary Record format that makes I/O blazing fast, and we'll use this format to train our model in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like image classification, face detections is a supervised training task. Here, the inputs are the images and the outputs are the bounding box coordinates.\n",
    "\n",
    "MXNet lets us convert images into a binary Record format that makes I/O blazing fast, and we'll use this format to train our model in the next step.\n",
    "\n",
    "Now that we understand our data and what the task is, we can launch a training job on SageMaker. To do that, we need to create our SageMaker session object and our execution role with the following methods that inherit the credentials/ARN from the notebook environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to put the data set on S3 so SageMaker can pull the data set onto the instances it spins up for training.\n",
    "\n",
    "The data has been pre-loaded into S3, so we'll use it's address as the location of our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \"s3://sagemaker-obj-det-demo/ssd-data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can easily launch a training job by defining a custom <tt>MXNet</tt> estimator, through which we specify custom training code and hyperparameters, as well as the type and number of instances we wish to train on at once. Tuning hyperparameters and launching distributed training jobs is as simple as changing these arguments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_sagemaker = MXNet(\"mxnet_ssd_face_detection.py\", \n",
    "                  role=role, \n",
    "                  train_instance_count=1, \n",
    "                  train_instance_type=\"ml.p2.xlarge\",\n",
    "                  sagemaker_session=sagemaker_session,\n",
    "                  hyperparameters={\n",
    "                                 'batch_size': 8, \n",
    "                                 'epochs': 1, \n",
    "                                 'learning_rate': 0.004, \n",
    "                                 'num_gpus': 1\n",
    "                                  })                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a single click, we can launch a training job, distributed or otherwise, and SageMaker completely handles the resource management to do so. \n",
    "\n",
    "When launching a training job, SageMaker:\n",
    "* Creates an S3 bucket to store training script <tt>mxnet_ssd_face_detection.py</tt>\n",
    "* Launches an EC2 instance(s) for training\n",
    "* Pulls down data in S3 and training script\n",
    "* Runs the training loop defined in the <tt>train_fn</tt> method in our training script, <tt>mxnet_ssd_face_detection.py</tt>\n",
    "* The model begins to train, logging the training loss and metrics every 200 iterations. We are only training for one epoch, but it is enough to see that the loss is indeed being minimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_sagemaker.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "\n",
    "We've trained our model here for a single epoch in the interest of time, but in reality we would want to run this overnight, which we've already done. After this, we'd make some modifications to our model to convert it from training format to inference.\n",
    "\n",
    "We've done this already, and will use SageMaker to deploy this model to an endpoint that will run inference on data requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxnet_model = MXNetModel(model_data=\"s3://sagemaker-obj-det-demo/model.tar.gz\",\n",
    "                         role=role, entry_point=\"mxnet_ssd_face_detection.py\",\n",
    "                        py_version='py3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the actual deployment will take only one click; SageMaker takes care of launching the instances and the server itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = mxnet_model.deploy(instance_type=\"ml.c4.xlarge\", initial_instance_count=1)\n",
    "predictor.serializer = None\n",
    "predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've deployed the inference endpoint, we can start calling it by sending requests with serialized image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = '19_Couple_Couple_19_187.jpg'\n",
    "with open(test_file, mode='rb') as f: \n",
    "    img_buffer = f.read()\n",
    "response = predictor.predict(img_buffer.encode('base64'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response we recieve is a series of possible face detections:\n",
    "<tt>[id, score, xmin, ymin, xmax, ymax]</tt>\n",
    "\n",
    "We only have one class so <tt>id</tt> will always be zero. However, we can set a threshold for what level of confidence we want to have to include face detections as valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(response, test_file, thresh=0.5):\n",
    "    dets = pickle.loads(response)\n",
    "    dets = dets[0]\n",
    "    img = cv2.imread(test_file)\n",
    "    img[:, :, (0, 1, 2)] = img[:, :, (2, 1, 0)]\n",
    "    classes = ['face']\n",
    "    plt.imshow(img)\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    colors = dict()\n",
    "    for i in range(dets.shape[0]):\n",
    "        cls_id = int(dets[i, 0])\n",
    "        if cls_id >= 0:\n",
    "            score = dets[i, 1]\n",
    "            if score > thresh:\n",
    "                if cls_id not in colors:\n",
    "                    colors[cls_id] = (random.random(), random.random(), random.random())\n",
    "                xmin = int(dets[i, 2] * width)\n",
    "                ymin = int(dets[i, 3] * height)\n",
    "                xmax = int(dets[i, 4] * width)\n",
    "                ymax = int(dets[i, 5] * height)\n",
    "                rect = plt.Rectangle((xmin, ymin), xmax - xmin,\n",
    "                                     ymax - ymin, fill=False,\n",
    "                                     edgecolor=colors[cls_id],\n",
    "                                     linewidth=3.5)\n",
    "                plt.gca().add_patch(rect)\n",
    "                class_name = str(cls_id)\n",
    "                if classes and len(classes) > cls_id:\n",
    "                    class_name = classes[cls_id]\n",
    "                plt.gca().text(xmin, ymin - 2,\n",
    "                                '{:s} {:.3f}'.format(class_name, score),\n",
    "                                bbox=dict(facecolor=colors[cls_id], alpha=0.5),\n",
    "                                fontsize=12, color='white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the bounding box around the identified face, as well as the confidence level of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(response, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_mxnet_p27",
   "language": "python",
   "name": "conda_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
